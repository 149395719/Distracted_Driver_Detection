{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'train'\n",
    "test_folder = 'test'\n",
    "my_train_folder='my_train'\n",
    "my_validation_folder = 'my_validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立训练集、验证集目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "#classes=[\"c0\",\"c1\",\"c2\",\"c3\",\"c4\",\"c5\",\"c6\",\"c7\",\"c8\",\"c9\"]\n",
    "import os\n",
    "if os.path.exists(\"my_validation\") is False:\n",
    "    os.mkdir(\"my_validation\")    \n",
    "    for j in range(num_classes):\n",
    "        path = os.path.join(my_validation_folder, 'c' + str(j))\n",
    "        os.mkdir(path)\n",
    "if os.path.exists(\"my_train\") is False:\n",
    "    os.mkdir(\"my_train\")    \n",
    "    for j in range(num_classes):\n",
    "        path = os.path.join(my_train_folder, 'c' + str(j))\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计目录下各类图片数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class 0 : 2489', 'class 1 : 2267', 'class 2 : 2317', 'class 3 : 2346', 'class 4 : 2326', 'class 5 : 2312', 'class 6 : 2325', 'class 7 : 2002', 'class 8 : 1911', 'class 9 : 2129'] 22424\n",
      "['class 0 : 2357', 'class 1 : 2132', 'class 2 : 2173', 'class 3 : 2211', 'class 4 : 2194', 'class 5 : 2179', 'class 6 : 2187', 'class 7 : 1870', 'class 8 : 1781', 'class 9 : 1995'] 21079\n",
      "['class 0 : 132', 'class 1 : 135', 'class 2 : 144', 'class 3 : 135', 'class 4 : 132', 'class 5 : 133', 'class 6 : 138', 'class 7 : 132', 'class 8 : 130', 'class 9 : 134'] 1345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([132, 135, 144, 135, 132, 133, 138, 132, 130, 134], 1345)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np \n",
    "def img_number(path,num_classes=10):\n",
    "    train_num_files=[]\n",
    "    for j in range(num_classes):\n",
    "        train_path = os.path.join(path, 'c' + str(j), '*.jpg')\n",
    "        train_files = glob.glob(train_path)\n",
    "        train_num_files.append(np.size(train_files))\n",
    "    train= [('class ' + str(i)+ ' : '+ str(train_num_files[i])) for i in range(10)] \n",
    "    print(train,sum(train_num_files))\n",
    "    return train_num_files,sum(train_num_files)  \n",
    "\n",
    "img_number(train_folder)    \n",
    "img_number(my_train_folder)\n",
    "img_number(my_validation_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取driver_imgs_list文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver_data():\n",
    "    dr_dict = {}\n",
    "    dr_set = set()\n",
    "    path = os.path.join('driver_imgs_list.csv')\n",
    "    print('Read drivers data')\n",
    "    f = open(path, 'r')\n",
    "    line = f.readline()\n",
    "    while (1):\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        arr = line.strip().split(',')   \n",
    "        dr_dict[arr[2]] = arr[0]\n",
    "        dr_set.add(arr[0])\n",
    "    f.close()\n",
    "    return dr_dict,dr_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立训练集、验证集图片(随机选取两位司机)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read drivers data\n",
      "p041 p052\n",
      "train\\c0\\*.jpg\n",
      "train\\c1\\*.jpg\n",
      "train\\c2\\*.jpg\n",
      "train\\c3\\*.jpg\n",
      "train\\c4\\*.jpg\n",
      "train\\c5\\*.jpg\n",
      "train\\c6\\*.jpg\n",
      "train\\c7\\*.jpg\n",
      "train\\c8\\*.jpg\n",
      "train\\c9\\*.jpg\n"
     ]
    }
   ],
   "source": [
    "driver_img,driver_id=get_driver_data()\n",
    "import random\n",
    "driver_id_1=(random.choice(list(driver_id)))\n",
    "driver_id_2=(random.choice(list(driver_id)))\n",
    "if driver_id_1==driver_id_2:\n",
    "    driver_id_2=(random.choice(list(driver_id)))\n",
    "print(driver_id_1,driver_id_2)    \n",
    "def creative_train_val_dataset(numclasses=10):\n",
    "    for j in range(num_classes):\n",
    "        train_path = os.path.join(train_folder, 'c' + str(j), '*.jpg')\n",
    "        print(train_path)\n",
    "        train_files = glob.glob(train_path)\n",
    "        for eachfile in train_files:\n",
    "            v_path=os.path.join(my_validation_folder, 'c' + str(j))\n",
    "            t_path=os.path.join(my_train_folder, 'c' + str(j))\n",
    "            if driver_img[eachfile.split('\\\\')[-1].split('.')[0]+'.jpg'] in (driver_id_1,driver_id_2):\n",
    "                os.system (\"copy %s %s\" % (eachfile, v_path))\n",
    "            else:\n",
    "                os.system (\"copy %s %s\" % (eachfile, t_path))\n",
    "creative_train_val_dataset(numclasses=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class 0 : 2489', 'class 1 : 2267', 'class 2 : 2317', 'class 3 : 2346', 'class 4 : 2326', 'class 5 : 2312', 'class 6 : 2325', 'class 7 : 2002', 'class 8 : 1911', 'class 9 : 2129'] 22424\n",
      "['class 0 : 2357', 'class 1 : 2132', 'class 2 : 2173', 'class 3 : 2211', 'class 4 : 2194', 'class 5 : 2179', 'class 6 : 2187', 'class 7 : 1870', 'class 8 : 1781', 'class 9 : 1995'] 21079\n",
      "['class 0 : 132', 'class 1 : 135', 'class 2 : 144', 'class 3 : 135', 'class 4 : 132', 'class 5 : 133', 'class 6 : 138', 'class 7 : 132', 'class 8 : 130', 'class 9 : 134'] 1345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([132, 135, 144, 135, 132, 133, 138, 132, 130, 134], 1345)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_number(train_folder)    \n",
    "img_number(my_train_folder)\n",
    "img_number(my_validation_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "图片预处理及增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Q\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg train/validation loading\n",
      "----------------------------\n",
      "\n",
      "Found 21079 images belonging to 10 classes.\n",
      "Found 1345 images belonging to 10 classes.\n",
      "resent50 train/validation loading\n",
      "---------------------------------\n",
      "\n",
      "Found 21079 images belonging to 10 classes.\n",
      "Found 1345 images belonging to 10 classes.\n",
      "inceptionv3 train/validation loading\n",
      "------------------------------------\n",
      "\n",
      "Found 21079 images belonging to 10 classes.\n",
      "Found 1345 images belonging to 10 classes.\n",
      "inceptionresnetv2 train/validation loading\n",
      "------------------------------------------\n",
      "\n",
      "Found 21079 images belonging to 10 classes.\n",
      "Found 1345 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16\n",
    "from keras.applications import resnet50\n",
    "from keras.applications import inception_v3\n",
    "from keras.applications import inception_resnet_v2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "img_height, img_width=(299,299)\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "vgg_train_datagen = ImageDataGenerator(preprocessing_function=vgg16.preprocess_input)\n",
    "vgg_validation_datagen = ImageDataGenerator(preprocessing_function=vgg16.preprocess_input)\n",
    "\n",
    "resnet50_train_datagen = ImageDataGenerator(preprocessing_function=resnet50.preprocess_input)\n",
    "resnet50_validation_datagen = ImageDataGenerator(preprocessing_function=resnet50.preprocess_input)\n",
    "\n",
    "inceptionv3_train_datagen = ImageDataGenerator(preprocessing_function=inception_v3.preprocess_input)\n",
    "inceptionv3_validation_datagen = ImageDataGenerator(preprocessing_function=inception_v3.preprocess_input)\n",
    "\n",
    "inceptionresnetv2_train_datagen = ImageDataGenerator(preprocessing_function=inception_resnet_v2.preprocess_input)\n",
    "inceptionresnetv2_validation_datagen = ImageDataGenerator(preprocessing_function=inception_resnet_v2.preprocess_input)\n",
    "\n",
    "print(\"vgg train/validation loading\")\n",
    "print(\"----------------------------\")\n",
    "print(\"\")\n",
    "vgg_train_generator = vgg_train_datagen.flow_from_directory(\n",
    "        my_train_folder,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "vgg_validation_generator = vgg_validation_datagen.flow_from_directory(\n",
    "        my_validation_folder,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "print(\"resent50 train/validation loading\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"\")\n",
    "resnet50_train_generator = resnet50_train_datagen.flow_from_directory(\n",
    "        my_train_folder,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "resnet50_validation_generator = resnet50_validation_datagen.flow_from_directory(\n",
    "        my_validation_folder,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "print(\"inceptionv3 train/validation loading\")\n",
    "print(\"------------------------------------\")\n",
    "print(\"\")\n",
    "inceptionv3_train_generator = inceptionv3_train_datagen.flow_from_directory(\n",
    "        my_train_folder,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "inceptionv3_validation_generator = inceptionv3_validation_datagen.flow_from_directory(\n",
    "        my_validation_folder,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "print(\"inceptionresnetv2 train/validation loading\")\n",
    "print(\"------------------------------------------\")\n",
    "print(\"\")\n",
    "inceptionresnetv2_train_generator = inceptionresnetv2_train_datagen.flow_from_directory(\n",
    "        my_train_folder,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "inceptionresnetv2_validation_generator = inceptionresnetv2_validation_datagen.flow_from_directory(\n",
    "        my_validation_folder,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D,Dense,Dropout\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "vgg_base_model = VGG16(weights='imagenet', include_top=False)\n",
    "resnet50_base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "inceptionv3_base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "inceptionresnetv2_base_model = InceptionResNetV2(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, layer in enumerate(vgg_base_model.layers):\n",
    "#   print(i, layer.name)\n",
    "#print(\"---------------------------------------\")\n",
    "#for i, layer in enumerate(resnet50_base_model.layers):\n",
    "#   print(i, layer.name)\n",
    "#print(\"---------------------------------------\")\n",
    "#for i, layer in enumerate(inceptionv3_base_model.layers):\n",
    "#   print(i, layer.name)\n",
    "#print(\"---------------------------------------\")\n",
    "#for i, layer in enumerate(inceptionresnetv2_base_model.layers):\n",
    "#   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立VGG16神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x_vgg = vgg_base_model.output\n",
    "x_vgg = GlobalAveragePooling2D()(x_vgg)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# let's add a Dropout layer\n",
    "x_vgg = Dropout(0.2)(x_vgg)\n",
    "\n",
    "# and a logistic layer -- let's say we have 10 classes\n",
    "predictions = Dense(10, activation='softmax')(x_vgg)\n",
    "\n",
    "# this is the model we will train\n",
    "vgg_model = Model(inputs=vgg_base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "#for layer in vgg_base_model.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:    \n",
    "for layer in vgg_base_model.layers[:15]:\n",
    "   layer.trainable = False\n",
    "for layer in vgg_base_model.layers[15:]:\n",
    "   layer.trainable = True    \n",
    "sgd = optimizers.SGD(lr=1e-4, momentum=0.9)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "vgg_model.compile(optimizer=sgd, loss='categorical_crossentropy',metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x_resnet = resnet50_base_model.output\n",
    "x_resnet = GlobalAveragePooling2D()(x_resnet)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# let's add a Dropout layer\n",
    "x_resnet = Dropout(0.2)(x_resnet)\n",
    "\n",
    "# and a logistic layer -- let's say we have 10 classes\n",
    "predictions = Dense(10, activation='softmax')(x_resnet)\n",
    "\n",
    "# this is the model we will train\n",
    "resnet_model = Model(inputs=resnet50_base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "#for layer in vgg_base_model.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:    \n",
    "for layer in resnet50_base_model.layers[:151]:\n",
    "   layer.trainable = False\n",
    "for layer in resnet50_base_model.layers[151:]:\n",
    "   layer.trainable = True    \n",
    "sgd = optimizers.SGD(lr=1e-3, momentum=0.9)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "resnet_model.compile(optimizer=sgd, loss='categorical_crossentropy',metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x_inceptionv3 = inceptionv3_base_model.output\n",
    "x_inceptionv3 = GlobalAveragePooling2D()(x_inceptionv3)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# let's add a Dropout layer\n",
    "x_inceptionv3 = Dropout(0.2)(x_inceptionv3)\n",
    "\n",
    "# and a logistic layer -- let's say we have 10 classes\n",
    "predictions = Dense(10, activation='softmax')(x_inceptionv3)\n",
    "\n",
    "# this is the model we will train\n",
    "inceptionv3_model = Model(inputs=inceptionv3_base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "#for layer in vgg_base_model.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:    \n",
    "for layer in inceptionv3_base_model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in inceptionv3_base_model.layers[249:]:\n",
    "   layer.trainable = True    \n",
    "sgd = optimizers.SGD(lr=1e-3, momentum=0.9)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "inceptionv3_model.compile(optimizer=sgd, loss='categorical_crossentropy',metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x_inceptionresnetv2 = inceptionresnetv2_base_model.output\n",
    "x_inceptionresnetv2 = GlobalAveragePooling2D()(x_inceptionresnetv2)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# let's add a Dropout layer\n",
    "x_inceptionresnetv2 = Dropout(0.2)(x_inceptionresnetv2)\n",
    "\n",
    "# and a logistic layer -- let's say we have 10 classes\n",
    "predictions = Dense(10, activation='softmax')(x_inceptionresnetv2)\n",
    "\n",
    "# this is the model we will train\n",
    "inceptionresnetv2_model = Model(inputs=inceptionresnetv2_base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "#for layer in vgg_base_model.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:    \n",
    "for layer in inceptionresnetv2_base_model.layers[:698]:\n",
    "   layer.trainable = False\n",
    "for layer in inceptionresnetv2_base_model.layers[698:]:\n",
    "   layer.trainable = True    \n",
    "sgd = optimizers.SGD(lr=1e-3, momentum=0.9)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "inceptionresnetv2_model.compile(optimizer=sgd, loss='categorical_crossentropy',metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "672/672 [==============================] - 355s 528ms/step - loss: 0.0908 - categorical_accuracy: 0.9760 - val_loss: 0.1358 - val_categorical_accuracy: 0.9707\n",
      "Epoch 2/20\n",
      "672/672 [==============================] - 354s 527ms/step - loss: 0.0582 - categorical_accuracy: 0.9859 - val_loss: 0.3907 - val_categorical_accuracy: 0.8781\n",
      "Epoch 3/20\n",
      "672/672 [==============================] - 352s 524ms/step - loss: 0.0411 - categorical_accuracy: 0.9900 - val_loss: 0.3321 - val_categorical_accuracy: 0.9013\n",
      "Epoch 4/20\n",
      "672/672 [==============================] - 354s 527ms/step - loss: 0.0308 - categorical_accuracy: 0.9931 - val_loss: 0.3481 - val_categorical_accuracy: 0.8936\n",
      "Epoch 5/20\n",
      "672/672 [==============================] - 353s 525ms/step - loss: 0.0239 - categorical_accuracy: 0.9950 - val_loss: 0.0941 - val_categorical_accuracy: 0.9829\n",
      "Epoch 6/20\n",
      "672/672 [==============================] - 352s 524ms/step - loss: 0.0204 - categorical_accuracy: 0.9952 - val_loss: 0.2845 - val_categorical_accuracy: 0.9184\n",
      "Epoch 7/20\n",
      "672/672 [==============================] - 353s 525ms/step - loss: 0.0163 - categorical_accuracy: 0.9962 - val_loss: 0.2908 - val_categorical_accuracy: 0.9154\n",
      "Epoch 8/20\n",
      "672/672 [==============================] - 353s 526ms/step - loss: 0.0141 - categorical_accuracy: 0.9975 - val_loss: 0.2190 - val_categorical_accuracy: 0.9316\n",
      "Epoch 9/20\n",
      "672/672 [==============================] - 353s 526ms/step - loss: 0.0104 - categorical_accuracy: 0.9980 - val_loss: 0.1194 - val_categorical_accuracy: 0.9758\n",
      "Epoch 10/20\n",
      "672/672 [==============================] - 353s 525ms/step - loss: 0.0115 - categorical_accuracy: 0.9973 - val_loss: 0.2694 - val_categorical_accuracy: 0.9225\n",
      "Epoch 11/20\n",
      "672/672 [==============================] - 353s 525ms/step - loss: 0.0093 - categorical_accuracy: 0.9982 - val_loss: 0.3010 - val_categorical_accuracy: 0.9094\n",
      "Epoch 12/20\n",
      "672/672 [==============================] - 353s 525ms/step - loss: 0.0082 - categorical_accuracy: 0.9981 - val_loss: 0.1910 - val_categorical_accuracy: 0.9385\n",
      "Epoch 13/20\n",
      "672/672 [==============================] - 353s 525ms/step - loss: 0.0068 - categorical_accuracy: 0.9988 - val_loss: 0.2153 - val_categorical_accuracy: 0.9507\n",
      "Epoch 14/20\n",
      "672/672 [==============================] - 353s 525ms/step - loss: 0.0066 - categorical_accuracy: 0.9990 - val_loss: 0.2989 - val_categorical_accuracy: 0.9154\n",
      "Epoch 15/20\n",
      "672/672 [==============================] - 353s 525ms/step - loss: 0.0060 - categorical_accuracy: 0.9988 - val_loss: 0.3791 - val_categorical_accuracy: 0.8963\n",
      "Epoch 16/20\n",
      "672/672 [==============================] - 353s 526ms/step - loss: 0.0050 - categorical_accuracy: 0.9993 - val_loss: 0.1817 - val_categorical_accuracy: 0.9443\n",
      "Epoch 17/20\n",
      "672/672 [==============================] - 353s 525ms/step - loss: 0.0056 - categorical_accuracy: 0.9988 - val_loss: 0.2392 - val_categorical_accuracy: 0.9396\n",
      "Epoch 18/20\n",
      "672/672 [==============================] - 353s 526ms/step - loss: 0.0053 - categorical_accuracy: 0.9990 - val_loss: 0.3719 - val_categorical_accuracy: 0.8943\n",
      "Epoch 19/20\n",
      "672/672 [==============================] - 353s 525ms/step - loss: 0.0042 - categorical_accuracy: 0.9994 - val_loss: 0.2950 - val_categorical_accuracy: 0.9134\n",
      "Epoch 20/20\n",
      "672/672 [==============================] - 353s 526ms/step - loss: 0.0039 - categorical_accuracy: 0.9995 - val_loss: 0.1099 - val_categorical_accuracy: 0.9648\n",
      "Epoch 1/20\n",
      "672/672 [==============================] - 465s 692ms/step - loss: 0.0270 - categorical_accuracy: 0.9952 - val_loss: 0.8744 - val_categorical_accuracy: 0.7490\n",
      "Epoch 2/20\n",
      "672/672 [==============================] - 466s 693ms/step - loss: 0.0094 - categorical_accuracy: 0.9987 - val_loss: 1.2080 - val_categorical_accuracy: 0.6093\n",
      "Epoch 3/20\n",
      "672/672 [==============================] - 465s 692ms/step - loss: 0.0044 - categorical_accuracy: 0.9998 - val_loss: 1.1274 - val_categorical_accuracy: 0.6556\n",
      "Epoch 4/20\n",
      "672/672 [==============================] - 465s 693ms/step - loss: 0.0029 - categorical_accuracy: 0.9997 - val_loss: 0.8811 - val_categorical_accuracy: 0.7168\n",
      "Epoch 5/20\n",
      "672/672 [==============================] - 464s 691ms/step - loss: 0.0022 - categorical_accuracy: 0.9999 - val_loss: 0.7882 - val_categorical_accuracy: 0.7855\n",
      "Epoch 6/20\n",
      "672/672 [==============================] - 465s 692ms/step - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 1.4513 - val_categorical_accuracy: 0.5740\n",
      "Epoch 7/20\n",
      "672/672 [==============================] - 465s 691ms/step - loss: 0.0015 - categorical_accuracy: 0.9999 - val_loss: 1.2944 - val_categorical_accuracy: 0.6415\n",
      "Epoch 8/20\n",
      "672/672 [==============================] - 466s 693ms/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 0.9258 - val_categorical_accuracy: 0.6924\n",
      "Epoch 9/20\n",
      "672/672 [==============================] - 464s 690ms/step - loss: 9.4727e-04 - categorical_accuracy: 1.0000 - val_loss: 1.0474 - val_categorical_accuracy: 0.6888\n",
      "Epoch 10/20\n",
      "672/672 [==============================] - 464s 690ms/step - loss: 8.9927e-04 - categorical_accuracy: 1.0000 - val_loss: 1.4213 - val_categorical_accuracy: 0.5861\n",
      "Epoch 11/20\n",
      "672/672 [==============================] - 464s 691ms/step - loss: 7.3010e-04 - categorical_accuracy: 1.0000 - val_loss: 1.2352 - val_categorical_accuracy: 0.6495\n",
      "Epoch 12/20\n",
      "672/672 [==============================] - 465s 692ms/step - loss: 6.9245e-04 - categorical_accuracy: 1.0000 - val_loss: 0.7420 - val_categorical_accuracy: 0.7441\n",
      "Epoch 13/20\n",
      "672/672 [==============================] - 465s 691ms/step - loss: 6.5092e-04 - categorical_accuracy: 1.0000 - val_loss: 1.1046 - val_categorical_accuracy: 0.6928\n",
      "Epoch 14/20\n",
      "672/672 [==============================] - 464s 691ms/step - loss: 5.2584e-04 - categorical_accuracy: 1.0000 - val_loss: 1.4969 - val_categorical_accuracy: 0.5700\n",
      "Epoch 15/20\n",
      "672/672 [==============================] - 464s 691ms/step - loss: 4.5051e-04 - categorical_accuracy: 1.0000 - val_loss: 1.2540 - val_categorical_accuracy: 0.6516\n",
      "Epoch 16/20\n",
      "672/672 [==============================] - 466s 693ms/step - loss: 4.8453e-04 - categorical_accuracy: 1.0000 - val_loss: 0.6439 - val_categorical_accuracy: 0.7822\n",
      "Epoch 17/20\n",
      "672/672 [==============================] - 466s 693ms/step - loss: 4.5913e-04 - categorical_accuracy: 1.0000 - val_loss: 1.2682 - val_categorical_accuracy: 0.6566\n",
      "Epoch 18/20\n",
      "672/672 [==============================] - 465s 692ms/step - loss: 4.2565e-04 - categorical_accuracy: 1.0000 - val_loss: 1.4754 - val_categorical_accuracy: 0.5841\n",
      "Epoch 19/20\n",
      "672/672 [==============================] - 466s 693ms/step - loss: 3.8383e-04 - categorical_accuracy: 1.0000 - val_loss: 1.2809 - val_categorical_accuracy: 0.6546\n",
      "Epoch 20/20\n",
      "672/672 [==============================] - 469s 698ms/step - loss: 3.2074e-04 - categorical_accuracy: 1.0000 - val_loss: 0.5659 - val_categorical_accuracy: 0.7979\n",
      "Epoch 1/20\n",
      "672/672 [==============================] - 480s 715ms/step - loss: 0.0642 - categorical_accuracy: 0.9891 - val_loss: 2.2636 - val_categorical_accuracy: 0.2744\n",
      "Epoch 2/20\n",
      "672/672 [==============================] - 486s 723ms/step - loss: 0.0252 - categorical_accuracy: 0.9969 - val_loss: 2.4225 - val_categorical_accuracy: 0.2779\n",
      "Epoch 3/20\n",
      "672/672 [==============================] - 486s 724ms/step - loss: 0.0128 - categorical_accuracy: 0.9993 - val_loss: 2.9935 - val_categorical_accuracy: 0.1762\n",
      "Epoch 4/20\n",
      "672/672 [==============================] - 482s 717ms/step - loss: 0.0091 - categorical_accuracy: 0.9994 - val_loss: 2.9127 - val_categorical_accuracy: 0.1904\n",
      "Epoch 5/20\n",
      "672/672 [==============================] - 481s 716ms/step - loss: 0.0064 - categorical_accuracy: 0.9996 - val_loss: 2.5347 - val_categorical_accuracy: 0.2759\n",
      "Epoch 6/20\n",
      "672/672 [==============================] - 481s 716ms/step - loss: 0.0048 - categorical_accuracy: 0.9998 - val_loss: 2.6499 - val_categorical_accuracy: 0.2729\n",
      "Epoch 7/20\n",
      "672/672 [==============================] - 480s 715ms/step - loss: 0.0038 - categorical_accuracy: 1.0000 - val_loss: 3.0617 - val_categorical_accuracy: 0.2165\n",
      "Epoch 8/20\n",
      "672/672 [==============================] - 484s 720ms/step - loss: 0.0033 - categorical_accuracy: 1.0000 - val_loss: 2.8640 - val_categorical_accuracy: 0.2070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "672/672 [==============================] - 487s 724ms/step - loss: 0.0029 - categorical_accuracy: 0.9999 - val_loss: 2.6848 - val_categorical_accuracy: 0.2669\n",
      "Epoch 10/20\n",
      "672/672 [==============================] - 485s 722ms/step - loss: 0.0027 - categorical_accuracy: 0.9999 - val_loss: 2.8136 - val_categorical_accuracy: 0.2538\n",
      "Epoch 11/20\n",
      "672/672 [==============================] - 487s 724ms/step - loss: 0.0022 - categorical_accuracy: 1.0000 - val_loss: 3.1174 - val_categorical_accuracy: 0.2518\n",
      "Epoch 12/20\n",
      "672/672 [==============================] - 482s 717ms/step - loss: 0.0021 - categorical_accuracy: 1.0000 - val_loss: 2.8878 - val_categorical_accuracy: 0.2080\n",
      "Epoch 13/20\n",
      "672/672 [==============================] - 482s 717ms/step - loss: 0.0019 - categorical_accuracy: 1.0000 - val_loss: 2.8505 - val_categorical_accuracy: 0.2608\n",
      "Epoch 14/20\n",
      "672/672 [==============================] - 482s 718ms/step - loss: 0.0016 - categorical_accuracy: 1.0000 - val_loss: 2.9361 - val_categorical_accuracy: 0.2377\n",
      "Epoch 15/20\n",
      "672/672 [==============================] - 483s 718ms/step - loss: 0.0016 - categorical_accuracy: 1.0000 - val_loss: 3.0495 - val_categorical_accuracy: 0.2790\n",
      "Epoch 16/20\n",
      "672/672 [==============================] - 488s 726ms/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 2.9174 - val_categorical_accuracy: 0.2070\n",
      "Epoch 17/20\n",
      "672/672 [==============================] - 485s 721ms/step - loss: 0.0014 - categorical_accuracy: 1.0000 - val_loss: 2.8939 - val_categorical_accuracy: 0.2659\n",
      "Epoch 18/20\n",
      "672/672 [==============================] - 481s 716ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 3.1542 - val_categorical_accuracy: 0.2125\n",
      "Epoch 19/20\n",
      "672/672 [==============================] - 482s 718ms/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 2.9251 - val_categorical_accuracy: 0.3132\n",
      "Epoch 20/20\n",
      "672/672 [==============================] - 482s 717ms/step - loss: 0.0010 - categorical_accuracy: 1.0000 - val_loss: 2.8811 - val_categorical_accuracy: 0.2178\n",
      "Epoch 1/20\n",
      "672/672 [==============================] - 921s 1s/step - loss: 0.1304 - categorical_accuracy: 0.9630 - val_loss: 3.5094 - val_categorical_accuracy: 0.1582\n",
      "Epoch 2/20\n",
      "672/672 [==============================] - 924s 1s/step - loss: 0.0411 - categorical_accuracy: 0.9894 - val_loss: 3.7110 - val_categorical_accuracy: 0.1319\n",
      "Epoch 3/20\n",
      "672/672 [==============================] - 922s 1s/step - loss: 0.0207 - categorical_accuracy: 0.9948 - val_loss: 3.6379 - val_categorical_accuracy: 0.0554\n",
      "Epoch 4/20\n",
      "672/672 [==============================] - 922s 1s/step - loss: 0.0123 - categorical_accuracy: 0.9969 - val_loss: 3.1416 - val_categorical_accuracy: 0.1846\n",
      "Epoch 5/20\n",
      "672/672 [==============================] - 926s 1s/step - loss: 0.0094 - categorical_accuracy: 0.9975 - val_loss: 4.1460 - val_categorical_accuracy: 0.1752\n",
      "Epoch 6/20\n",
      "672/672 [==============================] - 920s 1s/step - loss: 0.0102 - categorical_accuracy: 0.9971 - val_loss: 4.4653 - val_categorical_accuracy: 0.0977\n",
      "Epoch 7/20\n",
      "672/672 [==============================] - 920s 1s/step - loss: 0.0068 - categorical_accuracy: 0.9984 - val_loss: 3.4046 - val_categorical_accuracy: 0.0987\n",
      "Epoch 8/20\n",
      "672/672 [==============================] - 923s 1s/step - loss: 0.0039 - categorical_accuracy: 0.9993 - val_loss: 3.5275 - val_categorical_accuracy: 0.1689\n",
      "Epoch 9/20\n",
      "672/672 [==============================] - 921s 1s/step - loss: 0.0044 - categorical_accuracy: 0.9990 - val_loss: 4.6251 - val_categorical_accuracy: 0.1722\n",
      "Epoch 10/20\n",
      "672/672 [==============================] - 920s 1s/step - loss: 0.0028 - categorical_accuracy: 0.9994 - val_loss: 5.2251 - val_categorical_accuracy: 0.0655\n",
      "Epoch 11/20\n",
      "672/672 [==============================] - 922s 1s/step - loss: 0.0033 - categorical_accuracy: 0.9993 - val_loss: 3.9591 - val_categorical_accuracy: 0.1229\n",
      "Epoch 12/20\n",
      "672/672 [==============================] - 930s 1s/step - loss: 0.0022 - categorical_accuracy: 0.9996 - val_loss: 4.0193 - val_categorical_accuracy: 0.1816\n",
      "Epoch 13/20\n",
      "672/672 [==============================] - 918s 1s/step - loss: 0.0016 - categorical_accuracy: 0.9998 - val_loss: 4.7804 - val_categorical_accuracy: 0.1702\n",
      "Epoch 14/20\n",
      "672/672 [==============================] - 916s 1s/step - loss: 0.0018 - categorical_accuracy: 0.9997 - val_loss: 5.0560 - val_categorical_accuracy: 0.0463\n",
      "Epoch 15/20\n",
      "672/672 [==============================] - 919s 1s/step - loss: 0.0017 - categorical_accuracy: 0.9996 - val_loss: 3.6017 - val_categorical_accuracy: 0.1601\n",
      "Epoch 16/20\n",
      "672/672 [==============================] - 917s 1s/step - loss: 0.0016 - categorical_accuracy: 0.9996 - val_loss: 4.0459 - val_categorical_accuracy: 0.1865\n",
      "Epoch 17/20\n",
      "672/672 [==============================] - 917s 1s/step - loss: 0.0021 - categorical_accuracy: 0.9996 - val_loss: 4.7498 - val_categorical_accuracy: 0.1611\n",
      "Epoch 18/20\n",
      "672/672 [==============================] - 916s 1s/step - loss: 0.0015 - categorical_accuracy: 0.9997 - val_loss: 4.7173 - val_categorical_accuracy: 0.0322\n",
      "Epoch 19/20\n",
      "672/672 [==============================] - 915s 1s/step - loss: 0.0015 - categorical_accuracy: 0.9998 - val_loss: 3.8685 - val_categorical_accuracy: 0.1883\n",
      "Epoch 20/20\n",
      "672/672 [==============================] - 918s 1s/step - loss: 0.0014 - categorical_accuracy: 0.9997 - val_loss: 4.3797 - val_categorical_accuracy: 0.1846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2177600c978>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "vgg_model.fit_generator(\n",
    "        vgg_train_generator,\n",
    "        steps_per_epoch=672,\n",
    "        epochs=20,\n",
    "        validation_data=vgg_validation_generator,\n",
    "        validation_steps=32)\n",
    "# fine-tune the model\n",
    "resnet_model.fit_generator(\n",
    "        resnet50_train_generator,\n",
    "        steps_per_epoch=672,\n",
    "        epochs=20,\n",
    "        validation_data=resnet50_validation_generator,\n",
    "        validation_steps=32)\n",
    "# fine-tune the model\n",
    "inceptionv3_model.fit_generator(\n",
    "        inceptionv3_train_generator,\n",
    "        steps_per_epoch=672,\n",
    "        epochs=20,\n",
    "        validation_data=inceptionv3_validation_generator,\n",
    "        validation_steps=32)\n",
    "inceptionresnetv2_model.fit_generator(\n",
    "        inceptionresnetv2_train_generator,\n",
    "        steps_per_epoch=672,\n",
    "        epochs=20,\n",
    "        validation_data=inceptionresnetv2_validation_generator,\n",
    "        validation_steps=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
