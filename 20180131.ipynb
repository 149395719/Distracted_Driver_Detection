{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'train'\n",
    "test_folder = 'test'\n",
    "my_train_folder='my_train'\n",
    "my_validation_folder = 'my_validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立训练集、验证集目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "#classes=[\"c0\",\"c1\",\"c2\",\"c3\",\"c4\",\"c5\",\"c6\",\"c7\",\"c8\",\"c9\"]\n",
    "import os\n",
    "if os.path.exists(\"my_validation\") is False:\n",
    "    os.mkdir(\"my_validation\")    \n",
    "    for j in range(num_classes):\n",
    "        path = os.path.join(my_validation_folder, 'c' + str(j))\n",
    "        os.mkdir(path)\n",
    "if os.path.exists(\"my_train\") is False:\n",
    "    os.mkdir(\"my_train\")    \n",
    "    for j in range(num_classes):\n",
    "        path = os.path.join(my_train_folder, 'c' + str(j))\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计目录下各类图片数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class 0 : 2489', 'class 1 : 2267', 'class 2 : 2317', 'class 3 : 2346', 'class 4 : 2326', 'class 5 : 2312', 'class 6 : 2325', 'class 7 : 2002', 'class 8 : 1911', 'class 9 : 2129'] 22424\n",
      "['class 0 : 2357', 'class 1 : 2132', 'class 2 : 2173', 'class 3 : 2211', 'class 4 : 2194', 'class 5 : 2179', 'class 6 : 2187', 'class 7 : 1870', 'class 8 : 1781', 'class 9 : 1995'] 21079\n",
      "['class 0 : 132', 'class 1 : 135', 'class 2 : 144', 'class 3 : 135', 'class 4 : 132', 'class 5 : 133', 'class 6 : 138', 'class 7 : 132', 'class 8 : 130', 'class 9 : 134'] 1345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([132, 135, 144, 135, 132, 133, 138, 132, 130, 134], 1345)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np \n",
    "def img_number(path,num_classes=10):\n",
    "    train_num_files=[]\n",
    "    for j in range(num_classes):\n",
    "        train_path = os.path.join(path, 'c' + str(j), '*.jpg')\n",
    "        train_files = glob.glob(train_path)\n",
    "        train_num_files.append(np.size(train_files))\n",
    "    train= [('class ' + str(i)+ ' : '+ str(train_num_files[i])) for i in range(10)] \n",
    "    print(train,sum(train_num_files))\n",
    "    return train_num_files,sum(train_num_files)  \n",
    "\n",
    "img_number(train_folder)    \n",
    "img_number(my_train_folder)\n",
    "img_number(my_validation_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取driver_imgs_list文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver_data():\n",
    "    dr_dict = {}\n",
    "    dr_set = set()\n",
    "    path = os.path.join('driver_imgs_list.csv')\n",
    "    print('Read drivers data')\n",
    "    f = open(path, 'r')\n",
    "    line = f.readline()\n",
    "    while (1):\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        arr = line.strip().split(',')   \n",
    "        dr_dict[arr[2]] = arr[0]\n",
    "        dr_set.add(arr[0])\n",
    "    f.close()\n",
    "    return dr_dict,dr_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立训练集、验证集图片(随机选取两位司机)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read drivers data\n",
      "p041 p052\n",
      "train\\c0\\*.jpg\n",
      "train\\c1\\*.jpg\n",
      "train\\c2\\*.jpg\n",
      "train\\c3\\*.jpg\n",
      "train\\c4\\*.jpg\n",
      "train\\c5\\*.jpg\n",
      "train\\c6\\*.jpg\n",
      "train\\c7\\*.jpg\n",
      "train\\c8\\*.jpg\n",
      "train\\c9\\*.jpg\n"
     ]
    }
   ],
   "source": [
    "driver_img,driver_id=get_driver_data()\n",
    "import random\n",
    "driver_id_1=(random.choice(list(driver_id)))\n",
    "driver_id_2=(random.choice(list(driver_id)))\n",
    "if driver_id_1==driver_id_2:\n",
    "    driver_id_2=(random.choice(list(driver_id)))\n",
    "print(driver_id_1,driver_id_2)    \n",
    "def creative_train_val_dataset(numclasses=10):\n",
    "    for j in range(num_classes):\n",
    "        train_path = os.path.join(train_folder, 'c' + str(j), '*.jpg')\n",
    "        print(train_path)\n",
    "        train_files = glob.glob(train_path)\n",
    "        for eachfile in train_files:\n",
    "            v_path=os.path.join(my_validation_folder, 'c' + str(j))\n",
    "            t_path=os.path.join(my_train_folder, 'c' + str(j))\n",
    "            if driver_img[eachfile.split('\\\\')[-1].split('.')[0]+'.jpg'] in (driver_id_1,driver_id_2):\n",
    "                os.system (\"copy %s %s\" % (eachfile, v_path))\n",
    "            else:\n",
    "                os.system (\"copy %s %s\" % (eachfile, t_path))\n",
    "creative_train_val_dataset(numclasses=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class 0 : 2489', 'class 1 : 2267', 'class 2 : 2317', 'class 3 : 2346', 'class 4 : 2326', 'class 5 : 2312', 'class 6 : 2325', 'class 7 : 2002', 'class 8 : 1911', 'class 9 : 2129'] 22424\n",
      "['class 0 : 2357', 'class 1 : 2132', 'class 2 : 2173', 'class 3 : 2211', 'class 4 : 2194', 'class 5 : 2179', 'class 6 : 2187', 'class 7 : 1870', 'class 8 : 1781', 'class 9 : 1995'] 21079\n",
      "['class 0 : 132', 'class 1 : 135', 'class 2 : 144', 'class 3 : 135', 'class 4 : 132', 'class 5 : 133', 'class 6 : 138', 'class 7 : 132', 'class 8 : 130', 'class 9 : 134'] 1345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([132, 135, 144, 135, 132, 133, 138, 132, 130, 134], 1345)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_number(train_folder)    \n",
    "img_number(my_train_folder)\n",
    "img_number(my_validation_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "图片预处理及增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Q\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21079 images belonging to 10 classes.\n",
      "Found 1345 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import inception_resnet_v2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "img_height, img_width=(299,299)\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "        #samplewise_center=True,\n",
    "        #rescale=1./127.5,\n",
    "        rotation_range = 20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        #shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        #horizontal_flip=True,\n",
    "        #fill_mode='nearest',\n",
    "        preprocessing_function=inception_resnet_v2.preprocess_input)\n",
    "        #data_format=\"channels_last\")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(#samplewise_center=True,\n",
    "                                        preprocessing_function=inception_resnet_v2.preprocess_input)\n",
    "                                        #data_format=\"channels_last\")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        my_train_folder,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        my_validation_folder,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "base_model = InceptionResNetV2(weights='imagenet', include_top=False)\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D,Dense,Dropout\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# let's add a Dropout layer\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# and a logistic layer -- let's say we have 10 classes\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy',metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "672/672 [==============================] - 391s 582ms/step - loss: 2.1841 - categorical_accuracy: 0.2173 - val_loss: 2.5050 - val_categorical_accuracy: 0.0908\n",
      "Epoch 2/64\n",
      "672/672 [==============================] - 378s 562ms/step - loss: 1.9823 - categorical_accuracy: 0.3462 - val_loss: 2.6305 - val_categorical_accuracy: 0.1084\n",
      "Epoch 3/64\n",
      "672/672 [==============================] - 376s 559ms/step - loss: 1.8549 - categorical_accuracy: 0.4019 - val_loss: 2.5769 - val_categorical_accuracy: 0.1250\n",
      "Epoch 4/64\n",
      "672/672 [==============================] - 378s 562ms/step - loss: 1.7531 - categorical_accuracy: 0.4433 - val_loss: 2.7756 - val_categorical_accuracy: 0.1299\n",
      "Epoch 5/64\n",
      "672/672 [==============================] - 395s 587ms/step - loss: 1.6831 - categorical_accuracy: 0.4616 - val_loss: 2.9493 - val_categorical_accuracy: 0.1094\n",
      "Epoch 6/64\n",
      "672/672 [==============================] - 374s 557ms/step - loss: 1.6271 - categorical_accuracy: 0.4794 - val_loss: 2.9744 - val_categorical_accuracy: 0.1367\n",
      "Epoch 7/64\n",
      "672/672 [==============================] - 374s 557ms/step - loss: 1.5856 - categorical_accuracy: 0.4972 - val_loss: 2.7845 - val_categorical_accuracy: 0.1445\n",
      "Epoch 8/64\n",
      "672/672 [==============================] - 375s 557ms/step - loss: 1.5496 - categorical_accuracy: 0.5021 - val_loss: 3.1087 - val_categorical_accuracy: 0.1260\n",
      "Epoch 9/64\n",
      "672/672 [==============================] - 374s 557ms/step - loss: 1.5114 - categorical_accuracy: 0.5149 - val_loss: 2.9333 - val_categorical_accuracy: 0.1338\n",
      "Epoch 10/64\n",
      "672/672 [==============================] - 375s 557ms/step - loss: 1.4944 - categorical_accuracy: 0.5206 - val_loss: 3.0940 - val_categorical_accuracy: 0.1338\n",
      "Epoch 11/64\n",
      "672/672 [==============================] - 379s 564ms/step - loss: 1.4661 - categorical_accuracy: 0.5288 - val_loss: 2.9929 - val_categorical_accuracy: 0.1484\n",
      "Epoch 12/64\n",
      "672/672 [==============================] - 381s 567ms/step - loss: 1.4367 - categorical_accuracy: 0.5376 - val_loss: 2.8826 - val_categorical_accuracy: 0.1582\n",
      "Epoch 13/64\n",
      "672/672 [==============================] - 380s 566ms/step - loss: 1.4195 - categorical_accuracy: 0.5387 - val_loss: 2.9552 - val_categorical_accuracy: 0.1514\n",
      "Epoch 14/64\n",
      "672/672 [==============================] - 381s 568ms/step - loss: 1.4032 - categorical_accuracy: 0.5453 - val_loss: 2.9857 - val_categorical_accuracy: 0.1582\n",
      "Epoch 15/64\n",
      "672/672 [==============================] - 384s 571ms/step - loss: 1.3881 - categorical_accuracy: 0.5501 - val_loss: 3.1896 - val_categorical_accuracy: 0.1592\n",
      "Epoch 16/64\n",
      "672/672 [==============================] - 384s 572ms/step - loss: 1.3746 - categorical_accuracy: 0.5551 - val_loss: 3.1019 - val_categorical_accuracy: 0.1553\n",
      "Epoch 17/64\n",
      "672/672 [==============================] - 384s 571ms/step - loss: 1.3743 - categorical_accuracy: 0.5550 - val_loss: 2.9099 - val_categorical_accuracy: 0.1699\n",
      "Epoch 18/64\n",
      "672/672 [==============================] - 384s 571ms/step - loss: 1.3561 - categorical_accuracy: 0.5549 - val_loss: 2.9687 - val_categorical_accuracy: 0.1611\n",
      "Epoch 19/64\n",
      "672/672 [==============================] - 384s 571ms/step - loss: 1.3430 - categorical_accuracy: 0.5608 - val_loss: 2.9743 - val_categorical_accuracy: 0.1787\n",
      "Epoch 20/64\n",
      "672/672 [==============================] - 384s 572ms/step - loss: 1.3350 - categorical_accuracy: 0.5647 - val_loss: 3.1560 - val_categorical_accuracy: 0.1592\n",
      "Epoch 21/64\n",
      "672/672 [==============================] - 386s 574ms/step - loss: 1.3142 - categorical_accuracy: 0.5685 - val_loss: 3.1666 - val_categorical_accuracy: 0.1567\n",
      "Epoch 22/64\n",
      "672/672 [==============================] - 384s 572ms/step - loss: 1.3105 - categorical_accuracy: 0.5681 - val_loss: 3.2521 - val_categorical_accuracy: 0.1465\n",
      "Epoch 23/64\n",
      "672/672 [==============================] - 380s 565ms/step - loss: 1.3079 - categorical_accuracy: 0.5664 - val_loss: 3.2375 - val_categorical_accuracy: 0.1396\n",
      "Epoch 24/64\n",
      "672/672 [==============================] - 380s 566ms/step - loss: 1.3019 - categorical_accuracy: 0.5718 - val_loss: 3.0546 - val_categorical_accuracy: 0.1611\n",
      "Epoch 25/64\n",
      "672/672 [==============================] - 384s 571ms/step - loss: 1.3004 - categorical_accuracy: 0.5690 - val_loss: 3.0307 - val_categorical_accuracy: 0.1709\n",
      "Epoch 26/64\n",
      "672/672 [==============================] - 380s 565ms/step - loss: 1.2927 - categorical_accuracy: 0.5738 - val_loss: 3.0304 - val_categorical_accuracy: 0.1650\n",
      "Epoch 27/64\n",
      "672/672 [==============================] - 378s 563ms/step - loss: 1.2718 - categorical_accuracy: 0.5794 - val_loss: 3.0817 - val_categorical_accuracy: 0.1875\n",
      "Epoch 28/64\n",
      "672/672 [==============================] - 383s 571ms/step - loss: 1.2719 - categorical_accuracy: 0.5807 - val_loss: 3.0981 - val_categorical_accuracy: 0.1797\n",
      "Epoch 29/64\n",
      "671/672 [============================>.] - ETA: 0s - loss: 1.2735 - categorical_accuracy: 0.5763"
     ]
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=672,\n",
    "        epochs=64,\n",
    "        validation_data=train_generator,\n",
    "        validation_steps=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
